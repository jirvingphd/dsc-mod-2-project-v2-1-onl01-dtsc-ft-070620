{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Housing Linear Regression Analysis - Revisisted\n",
    "\n",
    "Please fill out:\n",
    "* Student name:  James M. Irving\n",
    "* Student pace:  full time\n",
    "* Scheduled project review date/time:  03/08/19 at 12pm\n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL: https://jirvingphd.github.io/harnessing_seaborn_subplots_for_eda\n",
    "___\n",
    "- Updated 08/27/20 for online-ds-ft-070620 Mod 2 Project Office Hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents (Links)<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#King-County-Housing-Linear-Regression-Analysis---Revisisted\" data-toc-modified-id=\"King-County-Housing-Linear-Regression-Analysis---Revisisted-1\">King County Housing Linear Regression Analysis - Revisisted</a></span></li><li><span><a href=\"#INTRODUCTION\" data-toc-modified-id=\"INTRODUCTION-2\">INTRODUCTION</a></span><ul class=\"toc-item\"><li><span><a href=\"#Goal/Purpose\" data-toc-modified-id=\"Goal/Purpose-2.1\">Goal/Purpose</a></span></li><li><span><a href=\"#Summary-Figures\" data-toc-modified-id=\"Summary-Figures-2.2\">Summary Figures</a></span></li><li><span><a href=\"#Outline-of-Data-Processing-and-Analysis(using-OSEMN-model)\" data-toc-modified-id=\"Outline-of-Data-Processing-and-Analysis(using-OSEMN-model)-2.3\">Outline of Data Processing and Analysis(using OSEMN model)</a></span></li></ul></li><li><span><a href=\"#OBTAIN:\" data-toc-modified-id=\"OBTAIN:-3\">OBTAIN:</a></span></li><li><span><a href=\"#SCRUB:\" data-toc-modified-id=\"SCRUB:-4\">SCRUB:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fill-in-null-values-and-recast-variables-for-EDA-by-columns\" data-toc-modified-id=\"Fill-in-null-values-and-recast-variables-for-EDA-by-columns-4.1\">Fill in null values and recast variables for EDA by columns</a></span></li><li><span><a href=\"#Question-1:-Which-predictors-should-be-analyzed-as-continuous-data,-vs-binned/categorical-data?\" data-toc-modified-id=\"Question-1:-Which-predictors-should-be-analyzed-as-continuous-data,-vs-binned/categorical-data?-4.2\">Question 1: Which predictors should be analyzed as continuous data, vs binned/categorical data?</a></span></li><li><span><a href=\"#COARSE-BINNING-Numerical-Data\" data-toc-modified-id=\"COARSE-BINNING-Numerical-Data-4.3\">COARSE-BINNING Numerical Data</a></span></li><li><span><a href=\"#CHECKING-FOR-MULTICOLLINEARITY\" data-toc-modified-id=\"CHECKING-FOR-MULTICOLLINEARITY-4.4\">CHECKING FOR MULTICOLLINEARITY</a></span></li></ul></li><li><span><a href=\"#EXPLORE:\" data-toc-modified-id=\"EXPLORE:-5\">EXPLORE:</a></span><ul class=\"toc-item\"><li><span><a href=\"#EDA-before-normalization/transformation\" data-toc-modified-id=\"EDA-before-normalization/transformation-5.1\">EDA before normalization/transformation</a></span></li></ul></li><li><span><a href=\"#[SCRUB-2]-NORMALIZING-&amp;-TRANSFORMING\" data-toc-modified-id=\"[SCRUB-2]-NORMALIZING-&amp;-TRANSFORMING-6\">[SCRUB-2] NORMALIZING &amp; TRANSFORMING</a></span><ul class=\"toc-item\"><li><span><a href=\"#Outlier-Removal---visualizing\" data-toc-modified-id=\"Outlier-Removal---visualizing-6.1\">Outlier Removal - visualizing</a></span></li><li><span><a href=\"#REMOVING-OUTLIERS\" data-toc-modified-id=\"REMOVING-OUTLIERS-6.2\">REMOVING OUTLIERS</a></span></li><li><span><a href=\"#NORMALIZING-UNITS-(RobustScaler)\" data-toc-modified-id=\"NORMALIZING-UNITS-(RobustScaler)-6.3\">NORMALIZING UNITS (RobustScaler)</a></span></li></ul></li><li><span><a href=\"#CHECKING-NORMALIZED-DATASET\" data-toc-modified-id=\"CHECKING-NORMALIZED-DATASET-7\">CHECKING NORMALIZED DATASET</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recheck-multipol\" data-toc-modified-id=\"Recheck-multipol-7.1\">Recheck multipol</a></span></li><li><span><a href=\"#CAT.CODES-FOR-BINNED-DATA\" data-toc-modified-id=\"CAT.CODES-FOR-BINNED-DATA-7.2\">CAT.CODES FOR BINNED DATA</a></span></li><li><span><a href=\"#Concatenate-final-df-for-modeling-(df_run)\" data-toc-modified-id=\"Concatenate-final-df-for-modeling-(df_run)-7.3\">Concatenate final df for modeling (df_run)</a></span></li></ul></li><li><span><a href=\"#FITTING-AN-INTIAL-MODEL:\" data-toc-modified-id=\"FITTING-AN-INTIAL-MODEL:-8\">FITTING AN INTIAL MODEL:</a></span><ul class=\"toc-item\"><li><span><a href=\"#DETERMINING-IDEAL-FEATURES-TO-USE\" data-toc-modified-id=\"DETERMINING-IDEAL-FEATURES-TO-USE-8.1\">DETERMINING IDEAL FEATURES TO USE</a></span></li><li><span><a href=\"#PRELIMINARY-UNIVARIATE-LINEAR-REGRESSION-MODELING\" data-toc-modified-id=\"PRELIMINARY-UNIVARIATE-LINEAR-REGRESSION-MODELING-8.2\">PRELIMINARY UNIVARIATE LINEAR REGRESSION MODELING</a></span></li><li><span><a href=\"#MULTIVARIATE-REGRESSIONS\" data-toc-modified-id=\"MULTIVARIATE-REGRESSIONS-8.3\">MULTIVARIATE REGRESSIONS</a></span></li><li><span><a href=\"#Cross-Validation-with-K-Fold-Test-Train-Splits:\" data-toc-modified-id=\"Cross-Validation-with-K-Fold-Test-Train-Splits:-8.4\">Cross-Validation with K-Fold Test-Train Splits:</a></span></li></ul></li><li><span><a href=\"#FINAL-REGRESSION-RESULTS\" data-toc-modified-id=\"FINAL-REGRESSION-RESULTS-9\">FINAL REGRESSION RESULTS</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Fold-valiation-with-OLS\" data-toc-modified-id=\"K-Fold-valiation-with-OLS-9.1\">K-Fold valiation with OLS</a></span></li><li><span><a href=\"#Q-Q-Plots\" data-toc-modified-id=\"Q-Q-Plots-9.2\">Q-Q Plots</a></span></li></ul></li><li><span><a href=\"#FINAL-MODEL---New\" data-toc-modified-id=\"FINAL-MODEL---New-10\">FINAL MODEL - New</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predictor-Coefficients-&amp;-Their-Affect-On-Sales-Price\" data-toc-modified-id=\"Predictor-Coefficients-&amp;-Their-Affect-On-Sales-Price-10.1\">Predictor Coefficients &amp; Their Affect On Sales Price</a></span></li><li><span><a href=\"#Future-Directions\" data-toc-modified-id=\"Future-Directions-10.2\">Future Directions</a></span></li></ul></li><li><span><a href=\"#APPENDIX\" data-toc-modified-id=\"APPENDIX-11\">APPENDIX</a></span><ul class=\"toc-item\"><li><span><a href=\"#SUMMARY-FIGURE-CODE-FOR-PRESENTATION\" data-toc-modified-id=\"SUMMARY-FIGURE-CODE-FOR-PRESENTATION-11.1\">SUMMARY FIGURE CODE FOR PRESENTATION</a></span></li><li><span><a href=\"#HOW-TO:-Tableau-Maps\" data-toc-modified-id=\"HOW-TO:-Tableau-Maps-11.2\">HOW TO: Tableau Maps</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal/Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **Our goal is to using Multiple Linear Regression to gain insights into what features of a home have the largest effect on its price.**\n",
    "- We will use our model to provide actionable recommendations to homeowners who want to increase their home's value.\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-1-final-project-online-ds-ft-021119/master/Final%20Figures/map_latlong_price.png\" width = 700 halign=center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-1-final-project-online-ds-ft-021119/master/Final%20Figures/summary_figure.png\" width=800 halign=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-1-final-project-online-ds-ft-021119/master/Final%20Figures/map_median_price.png\" width=700 halign=center>\n",
    "\n",
    "> ***Note: maps were made with the free software Tableau Public. See the Appendix section at the bottom of the notebook for a \"how-to\".***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Data Processing and Analysis(using OSEMN model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **OBTAIN:** \n",
    "    **Import data, inspect, check for datatypes to convert and null values**\n",
    "    - Display header and info\n",
    "    - Drop any unneeded columns (df.drop(['col1','col2'],axis=1)\n",
    "\n",
    "2. **SCRUB: cast data types, identify outliers, check for multicollinearity, normalize data**\n",
    "    - Check and cast data types\n",
    "        - [x] Check for #'s that are store as objects (df.info())\n",
    "            - when converting to #'s, look for odd values (like many 0's), or strings that can't be converted\n",
    "            - Decide how to deal weird/null values (df.unique(), df.isna().sum(), df.describe()-min/max, etc\n",
    "        - [x]  Check for categorical variables stored as integers\n",
    "    - [x] Check for missing values  (df.isna().sum())\n",
    "        - Can drop rows or colums\n",
    "        - For missing numeric data with median or bin/convert to categorical\n",
    "        - For missing categorical data: make NaN own category OR replace with most common category\n",
    "    - [X] Check for multicollinearity\n",
    "         - use seaborn to make correlation matrix plot [Evernote Link](https://www.evernote.com/l/AArNyaEwjA5JUL6I9PazHs_ts_hU-m7ja1I/) \n",
    "        - Good rule of thumb is anything over 0.75 corr is high, remove the variable that has the most correl with the largest # of variables\n",
    "    - [x] Normalize data (may want to do after some exploring)\n",
    "        - Most popular is Z-scoring (but won't fix skew) \n",
    "        - Can log-transform to fix skewed data\n",
    "    \n",
    "            \n",
    "3. **EXPLORE:Check distributions, outliers, etc**\n",
    "    - [ ] Check scales, ranges (df.describe())\n",
    "    - [x] Check histograms to get an idea of distributions (df.hist()) and dat transformations to perform\n",
    "        - Can also do kernel density estimates\n",
    "    - [x] Use scatterplots to check for linearity and possible categorical variables (df.plot(kind-'scatter')\n",
    "        - categoricals will look like vertical lines\n",
    "    - [x] Use pd.plotting.scatter_matrix to visualize possible relationships\n",
    "    - [x] Check for linearity\n",
    "\n",
    "   \n",
    "4. **FIT AN INITIAL MODEL:** \n",
    "    - Various forms, detail later...\n",
    "    - **Assessing the model:**\n",
    "        - Assess parameters (slope,intercept)\n",
    "        - Check if the model explains the variation in the data (RMSE, F, R_square)\n",
    "        - *Are the coeffs, slopes, intercepts in appropriate units?*\n",
    "        - *Whats the impact of collinearity? Can we ignore?*\n",
    "5. **Revise the fitted model**\n",
    "    - Multicollinearity is big issue for lin regression and cannot fully remove it\n",
    "    - Use the predictive ability of model to test it (like R2 and RMSE)\n",
    "    - Check for missed non-linearity\n",
    "6. **Holdout validation / Train/test split**\n",
    "    - use sklearn train_test_split \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN:\n",
    " - Import required packages, read in dataframe, and definefunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:52.563148Z",
     "start_time": "2020-08-27T18:01:47.026826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds v0.2.23 loaded.  Read the docs: https://fs-ds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row1_col1\" class=\"data row1 col1\" >fsds</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_623f741c_e88f_11ea_ad2e_4865ee12e626row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10613c8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Pandas .iplot() method activated.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U fsds\n",
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:52.629657Z",
     "start_time": "2020-08-27T18:01:52.564658Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.121795Z",
     "start_time": "2020-08-27T18:01:52.631616Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ji' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17740cde00b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'deleted ji'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ji' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17740cde00b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0m_functions_jirving\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imported ji\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSyntaxError\u001b[0m: invalid syntax (_functions_jirving.py, line 24)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17740cde00b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imported ji\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mji\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ji' is not defined"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    del ji\n",
    "    print('deleted ji')\n",
    "    import _functions_jirving as ji\n",
    "except:\n",
    "    import _functions_jirving as ji\n",
    "    print(\"imported ji\")\n",
    "finally:\n",
    "    help(ji)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.126941Z",
     "start_time": "2020-08-27T18:01:47.044Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('kc_house_data.csv',index_col='id')   \n",
    "\n",
    "# Set index, create dataframe for dropped variables with id as index for both\n",
    "drop_me =['date']\n",
    "df.drop(drop_me,axis=1,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### NOTE TO USER: SAVED FILES.\n",
    "- The below cell determines the output filepaths and should not be altered unless you intend to change those locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.127834Z",
     "start_time": "2020-08-27T18:01:47.050Z"
    }
   },
   "outputs": [],
   "source": [
    "folder = %pwd\n",
    "fig_filepath = folder+'/Figures/'\n",
    "data_filepath = folder+'/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial inspection of dataframe, datatypes, and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.128739Z",
     "start_time": "2020-08-27T18:01:47.058Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.129590Z",
     "start_time": "2020-08-27T18:01:47.064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display DataFrame Infro\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.130486Z",
     "start_time": "2020-08-27T18:01:47.069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for columns with null values (remember strings/objects are not counted here)\n",
    "res = df.isna().sum()\n",
    "print(f'Total Null Values: {res.sum()} out of {len(df)}')\n",
    "display(res[res>0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.131317Z",
     "start_time": "2020-08-27T18:01:47.074Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U missingno\n",
    "import missingno as ms\n",
    "ms.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - There are null values that that need to be addressed in:\n",
    "    - waterfront\n",
    "    - view\n",
    "    - yr_renovated\n",
    "- According to the missingo matrix, there does not seem to be a pattern across columns as to which rows are missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in null values and recast variables for EDA by columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From examining the df.info and df.isna().sum(), there is 1 numerical data column that is currently text/object data type. \n",
    "    - I will address these first since they would be excluded from preliminary visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.132139Z",
     "start_time": "2020-08-27T18:01:47.083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Recast zipcode as a category\n",
    "# df['zipcode'] = df['zipcode'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.133383Z",
     "start_time": "2020-08-27T18:01:47.088Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_column(df['zipcode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Will leave zipcode as numeric for now so it can be visualized during EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sqft_basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.134853Z",
     "start_time": "2020-08-27T18:01:47.096Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RECASTING SQFT_BASEMENT\n",
    "# Checking why sqft_basement might be an object:\n",
    "# df['sqft_basement'].value_counts().nlargest(10)\n",
    "ji.check_column(df['sqft_basement'],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **sqft_basement is current an object, needs to be converted to int**\n",
    "    - Need to ~~replace~~ drop the 454 '?' values\n",
    "    - There are a lot of 0's, for sqft_basement. Not sure if I should keep them in the dataset. I am for now.\n",
    "    - _Note: I originally replaced the ?'s with 0's, but am re-running the dataset with them dropped altogether._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.135820Z",
     "start_time": "2020-08-27T18:01:47.102Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sqft_basement'] = pd.to_numeric(df['sqft_basement'],errors='coerce')\n",
    "df.dropna(subset=['sqft_basement'],inplace=True)\n",
    "ji.check_column(df['sqft_basement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View\n",
    "- ~~Replace the 61 null values with appropriate value for data type~~\n",
    "- Drop the 61 view null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.136764Z",
     "start_time": "2020-08-27T18:01:47.108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for columns with null values (remember strings/objects are not counted here)\n",
    "res = df.isna().sum() / len(df)\n",
    "print(res[res>0])\n",
    "# Waterfront, view, yr_renovated have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.137644Z",
     "start_time": "2020-08-27T18:01:47.113Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_column(df['view'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.138612Z",
     "start_time": "2020-08-27T18:01:47.118Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop null values from view and re-check column\n",
    "df.dropna(subset=['view'],inplace=True)\n",
    "ji.check_column(df['view'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.139470Z",
     "start_time": "2020-08-27T18:01:47.123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert view to category and create a coded version called code_view\n",
    "# df['view'] = df['view'].astype('category')\n",
    "# df['code_view'] = df.view.cat.codes\n",
    "# df['code_view'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ultimately decided to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.140468Z",
     "start_time": "2020-08-27T18:01:47.130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop drop_me variable from main df, save in df_dropped\n",
    "drop_me = 'view'\n",
    "df.drop(drop_me,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waterfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.141304Z",
     "start_time": "2020-08-27T18:01:47.137Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_column(df['waterfront'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Waterfront null values will be filled with the mode, which is 0. \n",
    "- Will then convert to int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.142429Z",
     "start_time": "2020-08-27T18:01:47.143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert waterfront to category, replace null values with \"NaN\"' string to make it a category\n",
    "df.dropna(subset=['waterfront'],inplace=True)\n",
    "df['waterfront'] = df['waterfront'].astype('int')\n",
    "ji.check_column(df['waterfront'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yr_renovated  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- yr_renovated has 3754 null values\n",
    "    - Presumably this is because the homes were never renovate\n",
    "\n",
    "- Fill all NaN with 0.0's \n",
    "- convert to a binary `is_renovated` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make is_reno category (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.143436Z",
     "start_time": "2020-08-27T18:01:47.150Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_column(df['yr_renovated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.144300Z",
     "start_time": "2020-08-27T18:01:47.157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in 0.0 yr_renovated as np.nan temporarily\n",
    "df['yr_renovated'].fillna(0.0,inplace=True)\n",
    "\n",
    "# Recheck for null values\n",
    "ji.check_column(df['yr_renovated']) #df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.145322Z",
     "start_time": "2020-08-27T18:01:47.162Z"
    }
   },
   "outputs": [],
   "source": [
    "df['is_renovated'] = (df['yr_renovated'] > 0).astype(int)\n",
    "ji.check_column(df['is_renovated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.146063Z",
     "start_time": "2020-08-27T18:01:47.166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop drop_me variable from main df, save in df_dropped\n",
    "drop_me = 'yr_renovated'\n",
    "df.drop(drop_me,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRUBBING THUS FAR...\n",
    "- Removed null values by dropping na from sqft_basement and view. \n",
    "- Converted waterfront to category and made NaN its own separate category (since there were so many null vlaues) \n",
    "- Converted yr_renovated to is_reno simple 0 or 1 value\n",
    "- Recase zipcodes as category since there is no numerical relationship between zipcode values\n",
    "\n",
    "\n",
    "- Next to inspect distributions and scatterplots to identify which numerical columns may be better analyzed as categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.146902Z",
     "start_time": "2020-08-27T18:01:47.173Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.147688Z",
     "start_time": "2020-08-27T18:01:47.178Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Which predictors should be analyzed as continuous data, vs binned/categorical data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying numerical data better analyzed as categorical\n",
    "- Will examine histogram distributions and scatter plots vs price for each variable in df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.148516Z",
     "start_time": "2020-08-27T18:01:47.185Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histograms and scatterplots vs target variable price for all numerical columns in df (show up in .describe())\n",
    "fig = ji.plot_hist_scat(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1: How to treat each variable\n",
    "- Notes on histograms and scatterplots\n",
    "#### First, to comment on scatter plots that are indicative of categorical data:\n",
    "- **Columns to be analyzed as categorical data:** (cast as int for now, make sure its .~~astype('category').cat.as_ordered()~~ one-hot coded later before running regression\n",
    "    - Waterfront\n",
    "    - View\n",
    "    - Zipcode\n",
    "- **Columns that are ordinal data.** (e.g. 2 floors is indeed  double 1 floor)\n",
    "    - Floors \n",
    "    - Bedrooms\n",
    "    - Bathrooms\n",
    "    - Condition\n",
    "    - Grade\n",
    "- **Columns that may be best analyzed by binning and casting as categorical data**\n",
    "    - Yr_built\n",
    "    - is_reno\n",
    "        - classified as simply renovated or not.** \n",
    "    - sqft_basement\n",
    "    - sqft_above\n",
    "- **Numerical columns** (that may be best analyzed as such)\n",
    "    - All sqft categories\n",
    "    - price\n",
    "    - Note: moved sqft_basement to binned category to deal with 0's, also added sqft_above to accompany it \n",
    "- **Numerical columns that were dropped**\n",
    "    - id\n",
    "    - Lat\n",
    "    - Long\n",
    "    - Date\n",
    "    - yr_renovated -> is_reno\n",
    "    \n",
    "  \n",
    "#### Second, to comment on distributions\n",
    "- The target variable, price, seems a bit skewed and _may_ be better analyzed as log-transformed. \n",
    "    - Try both log-transformed and unaltered\n",
    "- All sqft columns seem to be skewed and should be transformed. (log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of vartypes/names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create tuples with columns names of categorical variables for easy extraction \n",
    "cat_vars = ('waterfront','view','zipcode')\n",
    "ord_vars = ('grade','condition','floors','bedrooms','bathrooms')\n",
    "vars_to_bin = ('yr_built','yr_renovated','sqft_above','sqft_basement')\n",
    "num_vars = ('sqft_living', 'sqft_lot','sqft_living15', 'sqft_lot15')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COARSE-BINNING Numerical Data \n",
    "- yr_built, yr_renovated\n",
    "- Added sqft_basement due to 0 values\n",
    "- Added sqft_above to accompany basement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binning yr_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.149393Z",
     "start_time": "2020-08-27T18:01:47.195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the min and max of the yr variables for binning range\n",
    "df['yr_built'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.150592Z",
     "start_time": "2020-08-27T18:01:47.202Z"
    }
   },
   "outputs": [],
   "source": [
    "## Bin yr_built then make yr_built category\n",
    "# yr_built min is 1900, max is 2015\n",
    "bins = list(range(1900,2030,10))\n",
    "\n",
    "df['yr_built'].replace(np.nan,0,inplace=True)\n",
    "bins_yrbuilt = pd.cut(df['yr_built'], bins,include_lowest=True) # Cut into bins\n",
    "# check_column(bins_yrbuilt)\n",
    "\n",
    "df['bins_yrbuilt'] = bins_yrbuilt.astype('category').cat.as_ordered() #.cat.as_ordered()\n",
    "\n",
    "# Inspecting the binned data counts\n",
    "ji.check_column(df['bins_yrbuilt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.151624Z",
     "start_time": "2020-08-27T18:01:47.206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop original \n",
    "drop_me = 'yr_built'\n",
    "df.drop(drop_me,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binning sqft_basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.152821Z",
     "start_time": "2020-08-27T18:01:47.213Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sqft_basement'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.153893Z",
     "start_time": "2020-08-27T18:01:47.217Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_column(df['sqft_basement'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.155268Z",
     "start_time": "2020-08-27T18:01:47.222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definine bins where bins 0-1 is its own interval\n",
    "bins=[-np.inf]\n",
    "[bins.append(x) for x in range(1,5500,500)]\n",
    "# bins\n",
    "\n",
    "# cut_basement = df['sqft_basement'].replace(0,np.nan)\n",
    "cut_basement = df['sqft_basement'].replace(np.nan,0).copy()\n",
    "# cut_basement = cut_basement.replace('NaN',0)\n",
    "\n",
    "bins_sqftbase = pd.cut(cut_basement, bins=bins, include_lowest=True) # Cut into bins\n",
    "df['bins_sqftbasement'] = bins_sqftbase.copy()\n",
    "\n",
    "# Cast as ordered category\n",
    "df['bins_sqftbasement'] = df['bins_sqftbasement'].astype('category').cat.as_ordered()\n",
    "\n",
    "# Check result\n",
    "ji.check_column(df['bins_sqftbasement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.156050Z",
     "start_time": "2020-08-27T18:01:47.227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop original \n",
    "drop_me = 'sqft_basement'\n",
    "df.drop(drop_me,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binning sqft_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.157015Z",
     "start_time": "2020-08-27T18:01:47.235Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_column(df['sqft_above'],10)\n",
    "df['sqft_above'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.157790Z",
     "start_time": "2020-08-27T18:01:47.240Z"
    }
   },
   "outputs": [],
   "source": [
    "# sqft_above \n",
    "# Bins to cover range seen above in .describe\n",
    "bins = list(range(0,9501,500))\n",
    "\n",
    "# cut_above = df['sqft_above'].replace(0,np.nan)\n",
    "bins_sqftabove = pd.cut(df['sqft_above'], bins=bins, include_lowest=True) # Cut into bins, including left edge \n",
    "ji.check_column(bins_sqftabove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.159208Z",
     "start_time": "2020-08-27T18:01:47.245Z"
    }
   },
   "outputs": [],
   "source": [
    "bins_sqftabove.replace(np.nan,'NaN',inplace=True)\n",
    "df['bins_sqftabove']=bins_sqftabove.astype('category').cat.as_ordered()\n",
    "\n",
    "ji.check_column(df['bins_sqftabove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.160358Z",
     "start_time": "2020-08-27T18:01:47.250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop original \n",
    "drop_me = 'sqft_above'\n",
    "df.drop(drop_me,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CHECKING FOR MULTICOLLINEARITY\n",
    "### Question 2:Which predictors are closely related and should be dropped?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.162528Z",
     "start_time": "2020-08-27T18:01:47.256Z"
    }
   },
   "outputs": [],
   "source": [
    "# from fsds_100719 import ihelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.163660Z",
     "start_time": "2020-08-27T18:01:47.263Z"
    }
   },
   "outputs": [],
   "source": [
    "# ihelp(multiplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.164494Z",
     "start_time": "2020-08-27T18:01:47.268Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop('price',axis=1).corr().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.165544Z",
     "start_time": "2020-08-27T18:01:47.273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot correlation  heatmaps for all data \n",
    "# pause\n",
    "to_drop = ['price']\n",
    "# multiplot(df.drop(to_drop,axis=1))\n",
    "ji.multiplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2: \n",
    "- Sqft_living is highly correlated with sqft_living15 and grade,\n",
    "- These correlations make sense sense since neighborhoods probably have similar construction.\n",
    "    - The r values are ~0.75 (threshold) and have sufficient intuitive rationale to keep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE:\n",
    "- Re-examining the basic stats and distributions of the data.\n",
    "- Decide on transformations to perform\n",
    "    - Normalize afterwards.\n",
    "- Visually examine for possible relationships "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA before normalization/transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine basic descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.166596Z",
     "start_time": "2020-08-27T18:01:47.282Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.167517Z",
     "start_time": "2020-08-27T18:01:47.287Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on basic statistics\n",
    "- Bedrooms has some very clear outliers (max is 33, but 75% quartile is only 4)\n",
    "    - May want to remove outliers after Z-scoring (Absolute Z-score > 3)\n",
    "- Same with bathrooms (8 is max, 75% quartile is only 2.5)\n",
    "- Same with sqft_living (max 13540, 75% quartile = 2550) \n",
    "- Also same with sqft_lot15, sqftliving15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing numerical data\n",
    "- Distributions and scatterplots\n",
    "- Note: May want to cast all categoricals as strings/categories for visualization\n",
    "    - Keeping as is for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.168451Z",
     "start_time": "2020-08-27T18:01:47.295Z"
    }
   },
   "outputs": [],
   "source": [
    "from fsds_100719.imports import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.169226Z",
     "start_time": "2020-08-27T18:01:47.300Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fs.ds.reload()\n",
    "# stop\n",
    "fs.ihelp(ji.plot_hist_scat_sns,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.170015Z",
     "start_time": "2020-08-27T18:01:47.305Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# INSPECTING NUMERICAL DATA DISTPLOTS + KDE\n",
    "ji.plot_hist_scat_sns(df,style='seaborn-poster',figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on distplots \n",
    "- Essentialy all numerical data seems to be at least slightly skewed.\n",
    "    - Do not think it is sufficient to log-transform the data and lose model interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.171539Z",
     "start_time": "2020-08-27T18:01:47.312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define list of all categorical variables \n",
    "list_cat_vars = ['zipcode', 'bins_yrbuilt', 'bins_sqftbasement', 'bins_sqftabove','condition','grade','code_view','code_waterfront']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.172762Z",
     "start_time": "2020-08-27T18:01:47.317Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Plotting scatterplots and boxplots for categorical data\n",
    "# # plt.style.use('seaborn')\n",
    "# list_cat_vars = ['zipcode', 'bins_yrbuilt', 'bins_sqftbasement', 'bins_sqftabove','condition','grade','code_view','code_waterfront']\n",
    "# def plot_cat_box_sns(df,target='price',figsize=(12,6)):\n",
    "#     for column in list_cat_vars:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ji.plot_cat_box_sns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question/Answer 3A: Which categorical variables show the greatest potential as predictors?\n",
    "\n",
    "#### ANSWER 3A:\n",
    "**Notes on categorical scatter plots**\n",
    "- grade seems to be strongly related to price (notice how the whole range of values seems to increase in price at higher grades.\n",
    "- Zipcodes look to differ quite a bit in terms of price.\n",
    "- Condition looks to be highly related to price (at least above condition level 2)\n",
    "- View does not look as strongly related to price as I expected.\n",
    "- Floors do not seem as related as expected\n",
    "- yr_built may have some relationship with price\n",
    "- yr_renovated has somewhat of a trend, but recent renovations buck this trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.173787Z",
     "start_time": "2020-08-27T18:01:47.324Z"
    }
   },
   "outputs": [],
   "source": [
    "# # INSPECTING REGRESSION PLOTS\n",
    "# plt.style.use('seaborn')\n",
    "\n",
    "# plot_vars=df.describe().columns\n",
    "\n",
    "# for column in plot_vars:\n",
    "# #     df_plot=df[column]\n",
    "# #     df_plot = df.loc[df[column]>0]\n",
    "#     plot= sns.regplot(df[column], df['price'],robust=False,marker='.') #kde=True,label = column+' histogram')\n",
    "# #     plot = sns.boxplot(df[column],df['price'])\n",
    "#     title = column+' linear regression'\n",
    "#     plt.title(title.title())\n",
    "# #     plt.legend()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.174636Z",
     "start_time": "2020-08-27T18:01:47.329Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SCRUB-2] NORMALIZING & TRANSFORMING \n",
    "**Graphing raw vs normalized results to decide if dataset should be normalized**\n",
    "- ~~log-transform and z-score numerical data.~~    \n",
    "- Am using detect_outlier function to apply Tukey's method for outlier remnoval based on IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal - visualizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Will be using pre-defined function detect_outliers(df,n,var_name)\n",
    "    - Returns index of rows containing outliers based in IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.175631Z",
     "start_time": "2020-08-27T18:01:47.338Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.176375Z",
     "start_time": "2020-08-27T18:01:47.342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define variables to plot vs tukey-cleaned outliers \n",
    "vars_to_norm = ['sqft_living','sqft_lot','sqft_living15',\n",
    "                'sqft_lot15','bedrooms','bathrooms']\n",
    "df.describe().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.177340Z",
     "start_time": "2020-08-27T18:01:47.347Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.stats import normaltest as normtest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robscaler = RobustScaler()\n",
    "\n",
    "# vars_to_norm = ['sqft_living','sqft_lot','sqft_living15','sqft_lot15','bedrooms','bathrooms']\n",
    "norm_results = [['column','K_square','p-val']]\n",
    "\n",
    "# Graph all potential normalizedvariables\n",
    "for var_name in df.describe():\n",
    "\n",
    "    var = df[var_name]\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1 = sns.distplot(var, norm_hist=True) \n",
    "    ax1.set_title('Raw '+var_name)\n",
    "    #robscaler.fit_transform(np.array(var).reshape(-1,1)\n",
    "    \n",
    "    # OUTLIER REMOVAL\n",
    "    outliers_to_drop = ji.detect_outliers(df,0,[var_name])\n",
    "    var_clean =df[var_name].drop(outliers_to_drop)\n",
    "    \n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    ax2 = sns.distplot(var_clean,norm_hist=True)\n",
    "#     ax2 = sns.distplot(robscaler.fit_transform(np.array(var_clean).reshape(-1,1)),norm_hist=True)\n",
    "    \n",
    "    ax2.set_title('Tukey Outliers Removed '+var_name) #+var)\n",
    "    ax2.set_xlabel('Scale')\n",
    "    stat, p = normtest(var_clean)\n",
    "#     norm_results.append([var_clean,stat, p])\n",
    "# norm_results = pd.DataFrame(norm_results[2:],columns=norm_results[0])#,index='columns')\n",
    "# norm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question/Answer 3B: Does removal of outliers improve the distrubtions?\n",
    "\n",
    "- The data is skewed by outliers.\n",
    "    - Comparing it to IQR-method outlier elimination reveals much improved results\n",
    "    - The distributions look much more reasonabile with outliers removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVING OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall stats observations from beginning of EDA:\n",
    "> #### Notes on basic statistics\n",
    ">- Bedrooms has some very clear outliers (max is 33, but 75% quartile is only 4)\n",
    ">    - May want to remove outliers after Z-scoring (Absolute Z-score > 3)\n",
    ">- Same with bathrooms (8 is max, 75% quartile is only 2.5)\n",
    ">- Same with sqft_living (max 13540, 75% quartile = 2550) \n",
    ">- Also same with sqft_lot15, sqftliving15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.178192Z",
     "start_time": "2020-08-27T18:01:47.355Z"
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE VARIABLES TO GET THE OUTLIERS FOR (based on observations)\n",
    "# vars_to_norm = ['price','bedrooms''sqft_living','sqft_lot','sqft_living15','sqft_lot15','bedrooms','bathrooms'\n",
    "vars_to_clean = ['price','bedrooms','sqft_living','bathrooms','sqft_living15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.179064Z",
     "start_time": "2020-08-27T18:01:47.362Z"
    }
   },
   "outputs": [],
   "source": [
    "# GET OUTLIER INDICES AND REPORT \n",
    "\n",
    "outliers_to_drop = {}\n",
    "\n",
    "for col in vars_to_clean:\n",
    "    outliers_to_drop[col] = ji.detect_outliers(df,0,[col])\n",
    "# outliers_to_drop.keys()\n",
    "# outliers_to_drop.values()\n",
    "\n",
    "# Print out # of outliers\n",
    "for k, v in outliers_to_drop.items():\n",
    "    print(f'col: {k} has {len(v)} outliers. ({round(len(v)/len(df),2)*100}%)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.179980Z",
     "start_time": "2020-08-27T18:01:47.366Z"
    }
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.180713Z",
     "start_time": "2020-08-27T18:01:47.373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Intialize df_norm with df's values\n",
    "df_norm=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.181430Z",
     "start_time": "2020-08-27T18:01:47.378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate throught outliers_to_drop dictionary to replace outliers with np.nan\n",
    "for k, v in outliers_to_drop.items():\n",
    "    df_norm.loc[v,k] = np.nan # axis=0,inplace=True)\n",
    "\n",
    "# Display null values\n",
    "df_norm.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.182289Z",
     "start_time": "2020-08-27T18:01:47.383Z"
    }
   },
   "outputs": [],
   "source": [
    "# REMOVING OUTLIERS FROM PRICE\n",
    "drop_col = 'price'\n",
    "\n",
    "print(f'for {drop_col}:')\n",
    "print(f'# of intial rows: {len(df_norm)}')\n",
    "df_norm.dropna(subset=['price'],inplace=True)\n",
    "print(f'# after dropping rows: {len(df_norm)}')\n",
    "print(f'\\nOutliers remaining: \\n{df_norm.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.183213Z",
     "start_time": "2020-08-27T18:01:47.388Z"
    }
   },
   "outputs": [],
   "source": [
    "# REMOVING OUTLIERS FROM BEDROOMS\n",
    "drop_col = 'bedrooms'\n",
    "\n",
    "print(f'for {drop_col}:')\n",
    "print(f'# of intial rows: {len(df_norm)}')\n",
    "df_norm.dropna(subset=[drop_col],inplace=True)\n",
    "print(f'# after dropping rows: {len(df_norm)}')\n",
    "print(f'\\nOutliers remaining: \\n{df_norm.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.184268Z",
     "start_time": "2020-08-27T18:01:47.392Z"
    }
   },
   "outputs": [],
   "source": [
    "# REMOVING OUTLIERS FROM PRICE\n",
    "\n",
    "drop_col = 'sqft_living'\n",
    "\n",
    "print(f'for {drop_col}:')\n",
    "print(f'# of intial rows: {len(df_norm)}')\n",
    "df_norm.dropna(subset=[drop_col],inplace=True)\n",
    "print(f'# after dropping rows: {len(df_norm)}')\n",
    "print(f'\\nOutliers remaining: \\n{df_norm.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.185337Z",
     "start_time": "2020-08-27T18:01:47.398Z"
    }
   },
   "outputs": [],
   "source": [
    "# REMOVING OUTLIERS FROM BATHROOMS\n",
    "drop_col = 'bathrooms'\n",
    "\n",
    "print(f'for {drop_col}:')\n",
    "print(f'# of intial rows: {len(df_norm)}')\n",
    "df_norm.dropna(subset=[drop_col],inplace=True)\n",
    "print(f'# after dropping rows: {len(df_norm)}')\n",
    "print(f'\\nOutliers remaining: \\n{df_norm.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.186128Z",
     "start_time": "2020-08-27T18:01:47.404Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_col = 'sqft_living15'\n",
    "\n",
    "print(f'for {drop_col}:')\n",
    "print(f'# of intial rows: {len(df_norm)}')\n",
    "df_norm.dropna(subset=[drop_col],inplace=True)\n",
    "print(f'# after dropping rows: {len(df_norm)}')\n",
    "print(f'\\nOutliers remaining: \\n{df_norm.isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZING UNITS (RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.186716Z",
     "start_time": "2020-08-27T18:01:47.409Z"
    }
   },
   "outputs": [],
   "source": [
    "# ADDING OUTLIER REMOVAL FROM preprocessing.RobuseScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robscaler = RobustScaler()\n",
    "robscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.187644Z",
     "start_time": "2020-08-27T18:01:47.414Z"
    }
   },
   "outputs": [],
   "source": [
    "vars_to_scale = ['sqft_living','sqft_lot','sqft_living15','sqft_lot15','bedrooms','bathrooms']\n",
    "\n",
    "for col in vars_to_scale:\n",
    "            \n",
    "    col_data = np.array(np.array(df_norm[col]))\n",
    "    res = robscaler.fit_transform(col_data.reshape(-1,1)) #,df['price'])\n",
    "    df_norm['sca_'+col] = res.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.188759Z",
     "start_time": "2020-08-27T18:01:47.419Z"
    }
   },
   "outputs": [],
   "source": [
    "# IF DROPPING VARS UNCOMMENT BELOW\n",
    "# df_norm.drop(vars_to_scale,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.189512Z",
     "start_time": "2020-08-27T18:01:47.424Z"
    }
   },
   "outputs": [],
   "source": [
    "df_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.190530Z",
     "start_time": "2020-08-27T18:01:47.430Z"
    }
   },
   "outputs": [],
   "source": [
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING NORMALIZED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.191688Z",
     "start_time": "2020-08-27T18:01:47.437Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.style.use('fivethirtyeight')\n",
    "ji.plot_hist_scat_sns(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.193003Z",
     "start_time": "2020-08-27T18:01:47.442Z"
    }
   },
   "outputs": [],
   "source": [
    "df_norm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recheck multipol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.195172Z",
     "start_time": "2020-08-27T18:01:47.449Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.multiplot(df_norm.filter(regex='sca',axis=1))\n",
    "plt.title('Scaled Data only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.196133Z",
     "start_time": "2020-08-27T18:01:47.454Z"
    }
   },
   "outputs": [],
   "source": [
    "X =df_norm.loc[:,~(df_norm.columns.str.startswith('sca'))]\n",
    "ji.multiplot(X.drop('price',axis=1))\n",
    "plt.title('Un-scaled Data')\n",
    "X=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No multicollinearity to worry about. Huzzah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.197182Z",
     "start_time": "2020-08-27T18:01:47.461Z"
    }
   },
   "outputs": [],
   "source": [
    "# DEFINING DATASET TO RUN (df_run)\n",
    "df_run = df_norm.copy()\n",
    "# df_run.to_csv(data_filepath+'df_run_pre_codes.csv')\n",
    "df_run.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAT.CODES FOR BINNED DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.198096Z",
     "start_time": "2020-08-27T18:01:47.467Z"
    }
   },
   "outputs": [],
   "source": [
    "df_run.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.199132Z",
     "start_time": "2020-08-27T18:01:47.472Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_filt = pd.DataFrame({})\n",
    "\n",
    "# df_filt = df_run.filter(regex=('bins_'),axis =1).copy()\n",
    "# df_filt['zipcode']=df_run['zipcode'].copy()\n",
    "# df_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.200122Z",
     "start_time": "2020-08-27T18:01:47.477Z"
    }
   },
   "outputs": [],
   "source": [
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.200951Z",
     "start_time": "2020-08-27T18:01:47.482Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Creating binned vars cat codes\n",
    "# for col in df_filt:\n",
    "#     df_filt['code'+col] = df_filt[col].cat.codes\n",
    "#     df_filt.drop(col,axis=1,inplace=True)\n",
    "# df_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.202159Z",
     "start_time": "2020-08-27T18:01:47.486Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_filt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate final df for modeling (df_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.202920Z",
     "start_time": "2020-08-27T18:01:47.492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate codebins from df_filt + df_run\n",
    "# df_run = pd.concat([df_run, df_filt],axis=1)  #).filter(regex=('code'))],axis=1)\n",
    "df_run.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.203934Z",
     "start_time": "2020-08-27T18:01:47.498Z"
    }
   },
   "outputs": [],
   "source": [
    "df_run.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clever line of code to select columns by name\n",
    "\n",
    "```python\n",
    "# Select columns that do not contain the string 'logZ'\n",
    "df_run =df_run.loc[:,~(df_run.columns.str.startswith('logZ'))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving/loading df_run after cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITTING AN INTIAL MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETERMINING IDEAL FEATURES TO USE \n",
    "\n",
    "- Use MinMaxScaler to get on same scale\n",
    "- Use RFE to find the best features\n",
    "- Get ranking of feature importance (from both scaled and unscaled data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.204743Z",
     "start_time": "2020-08-27T18:01:47.507Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.205738Z",
     "start_time": "2020-08-27T18:01:47.512Z"
    }
   },
   "outputs": [],
   "source": [
    "df_run.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.206866Z",
     "start_time": "2020-08-27T18:01:47.517Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns = [col.lower().replace(' ','_') for col in df.columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.207684Z",
     "start_time": "2020-08-27T18:01:47.523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define selector function combining RFE and linear regression\n",
    "linreg = LinearRegression()\n",
    "selector = RFE(linreg, n_features_to_select=1)\n",
    "\n",
    "# Drop already scaled variables for this feature testing\n",
    "X =df_run.loc[:,~(df_run.columns.str.startswith(('bins','zip')))]\n",
    "X = X.drop('price',axis=1)\n",
    "\n",
    "# RUNNING RFE ON THE UNSCALED DATA(DEMONSTRATION)\n",
    "Y = df_run['price']\n",
    "# Y = df_run['logz_price']\n",
    "# X = df_run.drop(['price'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.208649Z",
     "start_time": "2020-08-27T18:01:47.529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking X\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.209784Z",
     "start_time": "2020-08-27T18:01:47.534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run regressions on X,Y \n",
    "selector = selector.fit(X,Y)\n",
    "\n",
    "# Saving unscaled rankings for demo purposes\n",
    "no_scale = selector.ranking_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.210862Z",
     "start_time": "2020-08-27T18:01:47.539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale all variables to value between 0-1 to use RFE to determine which features are the most important for determining price?\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the data before running RFE\n",
    "print('Consider revisiting this step and dummy-coding zipcode.')\n",
    "\n",
    "# ONLY SCALE NON-CATEGORICAL, ONE-HOT CATEGORICAL\n",
    "scaler.fit(X,Y)\n",
    "scaled_data = scaler.transform(X)\n",
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.211823Z",
     "start_time": "2020-08-27T18:01:47.545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running RFE with scaled data\n",
    "selector = selector.fit(scaled_data, Y) \n",
    "scaled = selector.ranking_\n",
    "type(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.212770Z",
     "start_time": "2020-08-27T18:01:47.550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with the ranked values of each feature for both scaled and unscaled data\n",
    "best_features = pd.DataFrame({'columns':X.columns, 'scaled_rank' : scaled,'unscaled_rank':no_scale})\n",
    "best_features.set_index('columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.213825Z",
     "start_time": "2020-08-27T18:01:47.556Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display dataframe (sorted based on unscaled rank)\n",
    "best_features.sort_values('unscaled_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.214725Z",
     "start_time": "2020-08-27T18:01:47.561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the difference in feature importance between analyzing scaled and unscaled data. \n",
    "# For demonstration purposes.\n",
    "features = pd.DataFrame({'Columns':X.columns, 'Not_Scaled':no_scale, 'Scaled':scaled})\n",
    "# PLot the difference between \n",
    "ax = features.set_index('Columns').plot(kind = 'bar',figsize=(12,8))\n",
    "ax.set_title('Feature Importance Scaled vs. Not Scaled')\n",
    "ax.set_ylabel('Features Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using elbow plots to identify the best # of features to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.215759Z",
     "start_time": "2020-08-27T18:01:47.568Z"
    }
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot R_squared and MSE for Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.216819Z",
     "start_time": "2020-08-27T18:01:47.574Z"
    }
   },
   "outputs": [],
   "source": [
    "r_squared = []\n",
    "for x in range(1, len(X.columns)):\n",
    "    selector = RFE(linreg, n_features_to_select=x)\n",
    "    selector.fit(scaled_data, Y)\n",
    "    linreg.fit(X[X.columns[selector.support_]], Y)\n",
    "    r_sq = linreg.score(X[X.columns[selector.support_]], Y)\n",
    "    r_squared.append(r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.217747Z",
     "start_time": "2020-08-27T18:01:47.580Z"
    }
   },
   "outputs": [],
   "source": [
    "# r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.218879Z",
     "start_time": "2020-08-27T18:01:47.585Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse=[]\n",
    "for x in range(1, len(X.columns)):\n",
    "    selector = RFE(linreg,  n_features_to_select=x)\n",
    "    selector.fit(scaled_data, Y)\n",
    "    linreg.fit(X[X.columns[selector.support_]], Y)\n",
    "    y_pred = linreg.predict(X[X.columns[selector.support_]])\n",
    "    mse.append(mean_squared_error(Y, y_pred))\n",
    "# mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.219889Z",
     "start_time": "2020-08-27T18:01:47.590Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "fig = plt.figure(figsize=(12 ,6))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(range(1, len(X.columns)), r_squared)\n",
    "ax1.set_ylabel('R_Squared')\n",
    "ax1.set_xlabel('Number of Features')\n",
    "ax1.grid()\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(range(1,len(mse)+1), mse )\n",
    "ax2.set_ylabel('MSE')\n",
    "\n",
    "ax2.set_xlabel('Number of Features',fontsize=20)\n",
    "ax2.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot R_squared and MSE for Unscaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.221364Z",
     "start_time": "2020-08-27T18:01:47.597Z"
    }
   },
   "outputs": [],
   "source": [
    "r_squared = []\n",
    "for x in range(1, len(X.columns)):\n",
    "    selector = RFE(linreg, n_features_to_select=x)\n",
    "    selector.fit(X, Y)\n",
    "    linreg.fit(X[X.columns[selector.support_]], Y)\n",
    "    r_sq = linreg.score(X[X.columns[selector.support_]], Y)\n",
    "    r_squared.append(r_sq)\n",
    "\n",
    "    \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse=[]\n",
    "for x in range(1, len(X.columns)):\n",
    "    selector = RFE(linreg,  n_features_to_select=x)\n",
    "    selector.fit(X, Y)\n",
    "    linreg.fit(X[X.columns[selector.support_]], Y)\n",
    "    y_pred = linreg.predict(X[X.columns[selector.support_]])\n",
    "    mse.append(mean_squared_error(Y, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12 ,6))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(range(1, len(X.columns)), r_squared)\n",
    "ax1.set_ylabel('R_Squared')\n",
    "ax1.set_xlabel('Number of Features')\n",
    "ax1.grid()\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(range(1,len(mse)+1), mse )\n",
    "ax2.set_ylabel('MSE')\n",
    "\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.grid() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Features Based on Rankings\n",
    "- The above figure is indicating that right now my best possible R2 with the lowest # of features would be with 6 predictors (judging unscaled data). \n",
    "- Now examine the sorted best_features dataframe to see which 6 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.222678Z",
     "start_time": "2020-08-27T18:01:47.603Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = ['bedrooms','bathrooms']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for col in cat_cols:\n",
    "    new_df[col] = encoder.fit_transform(df[col])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.224251Z",
     "start_time": "2020-08-27T18:01:47.608Z"
    }
   },
   "outputs": [],
   "source": [
    "best_features.sort_values('scaled_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.225473Z",
     "start_time": "2020-08-27T18:01:47.613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pull out the 6 best features via ranking \n",
    "best_num_feat = 6\n",
    "selected_features = best_features.sort_values('unscaled_rank')[0:best_num_feat]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.226597Z",
     "start_time": "2020-08-27T18:01:47.618Z"
    }
   },
   "outputs": [],
   "source": [
    "# USING UNSCALED\n",
    "selected_features.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.227754Z",
     "start_time": "2020-08-27T18:01:47.624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the columns of X\n",
    "X[selected_features.index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRELIMINARY UNIVARIATE LINEAR REGRESSION MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.228676Z",
     "start_time": "2020-08-27T18:01:47.632Z"
    }
   },
   "outputs": [],
   "source": [
    "df_run.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.229627Z",
     "start_time": "2020-08-27T18:01:47.637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running simple linear regression for each predictor on its own\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "\n",
    "# log_price = np.log(df['price'])\n",
    "# df['log_price'] = log_price\n",
    "\n",
    "target_var = 'price'\n",
    "col_names = df_run.drop(['price'],axis=1).columns\n",
    "\n",
    "# Create results list for saving the output statstics for each predictor\n",
    "results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value' ]] \n",
    "\n",
    "for idx, val in enumerate(col_names): \n",
    "    \n",
    "    # Use the names of the columns to determine format of forumla  \n",
    "    if val.startswith('code'):\n",
    "        \n",
    "        df_run[val] = df_run[val].astype('category').cat.as_ordered() \n",
    "        f =f'{str(target_var)}~C({val})'\n",
    "        \n",
    "    elif val.startswith('bin'):\n",
    "        \n",
    "        df_run[val] = df_run[val].cat.as_ordered() \n",
    "        f =f'{str(target_var)}~C({val})'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        f =f'{str(target_var)}~{val}'\n",
    "        \n",
    "    # Run the ols models     \n",
    "    model = smf.ols(formula=f, data=df_run).fit() \n",
    "    model.summary()\n",
    "    \n",
    "    # Append results\n",
    "    results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1] ]) \n",
    "    \n",
    "# Turn results into dataframe with correct index and columns\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.columns = res_df.iloc[0]\n",
    "res_df=res_df[1:]\n",
    "res_df.set_index('ind_var',inplace=True)\n",
    "res_df.sort_values('r_squared',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.230713Z",
     "start_time": "2020-08-27T18:01:47.642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initial variables for modeling\n",
    "try_modeling = ['codezipcode', 'grade','sca_sqft_living', 'sca_sqft_living15']\n",
    "# Hmm...realized there are redundant versions of variables and am just selecting the correct versions to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES FOLLOWING PRELIMINARY LINEAR REGRESSIONS\n",
    "- Variables that had high R_square with logz_price:\n",
    "    - New results = ['codezipcode', 'grade','sca_sqft_living', 'sca_sqft_living15']\n",
    "\n",
    "```python\n",
    " try_modeling = try_modeling = ['codezipcode', 'grade','sca_sqft_living', 'sca_sqft_living16']\n",
    "\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTIVARIATE REGRESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.231807Z",
     "start_time": "2020-08-27T18:01:47.649Z"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.api as sms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # PUTTING TOGETHER THE PREDICTORS TO RUN IN THE REGRESSION\n",
    "# ## Last min dummy vars []'cat_grade','cat_zipcode','cat_view','cat_bins_sqft_above','cat_bins_sqft_basement']\n",
    "# dum_grades = pd.get_dummies(df_run['cat_grade'],prefix='gr').iloc[:,:-1]\n",
    "# dum_view = pd.get_dummies(df_run['cat_view'], prefix='view').iloc[:,:-1]\n",
    "# dum_sqft_above = pd.get_dummies(df_run['cat_bins_sqftabove'],prefix='sqftAb').iloc[:,:-1]\n",
    "# dum_sqft_base = pd.get_dummies(df_run['cat_bins_sqftbasement'],prefix='sqftBa').iloc[:,:-1]\n",
    "\n",
    "\n",
    "# RUNNING K-FOLD VALIDATION WITH STATSMODELS OLS.\n",
    "# X = df_run.drop(['price','logZ_price'],axis=1)\n",
    "# list_predictors = ['logZ_sqft_living','logZ_sqft_living15','bedrooms','bathrooms','floors']\n",
    "# scaler = MinMaxScaler()\n",
    "# sel_columns = selected_features.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.232780Z",
     "start_time": "2020-08-27T18:01:47.654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define X, Y \n",
    "X = df_run[try_modeling]\n",
    "# X.columns\n",
    "\n",
    "Y = df_run['price']\n",
    "# y = df_run['logZ_price']\n",
    "\n",
    "# Get a list of predictor names string \n",
    "list_predictors = [str(x) for x in X.columns]\n",
    "list_predictors.append('intercept')\n",
    "list_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.233802Z",
     "start_time": "2020-08-27T18:01:47.660Z"
    }
   },
   "outputs": [],
   "source": [
    "df_run.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.234955Z",
     "start_time": "2020-08-27T18:01:47.666Z"
    }
   },
   "outputs": [],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.236062Z",
     "start_time": "2020-08-27T18:01:47.671Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.237084Z",
     "start_time": "2020-08-27T18:01:47.676Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Comcatenate X,Y for OLS\n",
    "df_run_ols = pd.concat([Y,X],axis=1)\n",
    "\n",
    "# Import packages\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Enter equation for selected predictors: (use C to run as categorical) \n",
    "# f1 = 'price~C(codezipcode)+C(grade)+sca_sqft_living+sca_sqft_living15' # 0.8 r1 Adjusted\n",
    "f1 = 'price~C(codezipcode)+grade+sca_sqft_living+sca_sqft_living15' \n",
    "\n",
    "# Run model and report sumamry\n",
    "model = smf.ols(formula=f1, data=df_run_ols).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation with K-Fold Test-Train Splits:\n",
    "```python\n",
    "- f1 = 'price ~ C(codezipcode) + grade + sca_sqft_living + sca_sqft_living15' \n",
    "```\n",
    "- price ~ zipcode(category), grade~~(category)~~, sqft_living(scaled to median, RobustScaler) + sqft_living15 (scaled to median, RobustScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df_run_ols to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.237980Z",
     "start_time": "2020-08-27T18:01:47.683Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_run_ols.to_csv(data_filepath+'df_run_ols_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.238756Z",
     "start_time": "2020-08-27T18:01:47.688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize Q-Q Plots\n",
    "resid1=model.resid\n",
    "fig = sm.graphics.qqplot(resid1, dist=stats.norm, line='45', fit=True,marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpreting the Q-Q plot:\n",
    "    - The Q-Q plot looks a bit crazy and may indicate... outliers? \n",
    "    - The only thing I did not check for outliers in final model was zipcode.\n",
    "    - Will run cross-validation with test-train-split to help decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.239811Z",
     "start_time": "2020-08-27T18:01:47.696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing final dataset again.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Re-inspecting XY\n",
    "# plot_hist_scat(df_run_ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL REGRESSION RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## K-Fold valiation with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.240928Z",
     "start_time": "2020-08-27T18:01:47.704Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# k_fold_val_ols(X,y,k=10):\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics \n",
    "\n",
    "y = df_run['price']\n",
    "\n",
    "\n",
    "# Run 10-fold cross validation\n",
    "results = [['set#','R_square_train','MSE_train','R_square_test','MSE_test']]\n",
    "\n",
    "num_coeff = X.shape[1]\n",
    "\n",
    "list_predictors = [str(x) for x in X.columns]\n",
    "list_predictors.append('intercept') \n",
    "\n",
    "reg_params = [list_predictors]\n",
    "\n",
    "i=0\n",
    "k=10\n",
    "while i <(k+1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y) #,stratify=[cat_col_names])\n",
    "\n",
    "    data = pd.concat([X_train,y_train],axis=1)\n",
    "    f1 = 'price~C(codezipcode)+grade+sca_sqft_living+sca_sqft_living15' \n",
    "    model = smf.ols(formula=f1, data=data).fit()\n",
    "    model.summary()\n",
    "    \n",
    "    y_hat_train = model.predict(X_train)\n",
    "    y_hat_test = model.predict(X_test)\n",
    "\n",
    "    train_residuals = y_hat_train - y_train\n",
    "    test_residuals = y_hat_test - y_test\n",
    "\n",
    "        \n",
    "    train_mse = metrics.mean_squared_error(y_train, y_hat_train)\n",
    "    test_mse = metrics.mean_squared_error(y_test, y_hat_test)\n",
    "\n",
    "    R_sqare_train = metrics.r2_score(y_train,y_hat_train)\n",
    "    R_square_test = metrics.r2_score(y_test,y_hat_test)\n",
    "\n",
    "    results.append([i,R_sqare_train,train_mse,R_square_test,test_mse])\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "results = pd.DataFrame(results[1:],columns=results[0])\n",
    "results\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.241870Z",
     "start_time": "2020-08-27T18:01:47.709Z"
    }
   },
   "outputs": [],
   "source": [
    "display(results.round(3).style.hide_index().set_caption('K FOLD VALIDATION RESULTS:'))\n",
    "print(results.mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Q Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.242873Z",
     "start_time": "2020-08-27T18:01:47.715Z"
    }
   },
   "outputs": [],
   "source": [
    "resid1=model.resid\n",
    "fig = sm.graphics.qqplot(resid1, dist=stats.norm, line='45', fit=True,marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.244057Z",
     "start_time": "2020-08-27T18:01:47.721Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = pd.concat([df_run[try_modeling],df_run['price']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.245114Z",
     "start_time": "2020-08-27T18:01:47.727Z"
    }
   },
   "outputs": [],
   "source": [
    "results.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MODEL - New \n",
    "- For k=10 fold validation, with price as target variable:\n",
    "    - mean r_squared for the test sets was 0.797, with mean MSE = 8.158691e+09\n",
    "    \n",
    "  f1 = 'price~C(codezipcode)+C(grade)+sca_sqft_living+sca_sqft_living15' \n",
    "- Predictors in final model:\n",
    "    - 'Zipcode'\n",
    "    - 'grade'\n",
    "    - 'sqft_living'\n",
    "    - 'sqfr_living15'\n",
    "\n",
    "- My final model indicates that the size, location, and housing geade to be critical components in determining salesprice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Coefficients & Their Affect On Sales Price\n",
    "- Grade, sqft_living, and sqft_living15 all have straight-forward relationships with sales price, with positive coefficients.\n",
    "    - sqft_living is a larger component of the price (coefficient: 1.043e+05)\n",
    "    - grade (coeff: 3.679e+04)and sqft_living15 (coeff: 3.767e+04) have a similar magnitude of an effect on sales price \n",
    "- Zipcode is a bit trickier, as each zipcode has its own coefficient. \n",
    "    - Overall, zipcode has a positive coefficient/effect on price (to varying degrees)\n",
    "    - There are, however, a couple zipcodes that negatively impact sales price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "- **With more time I would have proceeded to perform the following steps:**\n",
    "    - Additional changes to the predictors in the model\n",
    "        - Trying to remove outliers from zipcodes \n",
    "    - Explored additional transformations to the data.\n",
    "        - I was trying to be conservative to keep the interpretability of my model intact. \n",
    "        - Log-transforming the data improved the distributions but made it more difficult to interpret. \n",
    "    - I would further tweak the quality of the visuals, particularly changing the x-tick labels and rotation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.245956Z",
     "start_time": "2020-08-27T18:01:47.736Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dropped.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.247001Z",
     "start_time": "2020-08-27T18:01:47.741Z"
    }
   },
   "outputs": [],
   "source": [
    "df_run.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.247894Z",
     "start_time": "2020-08-27T18:01:47.747Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_final_data=pd.concat([df_run, df_dropped[['lat','long','id']]],axis=1)\n",
    "df_final_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.248846Z",
     "start_time": "2020-08-27T18:01:47.752Z"
    }
   },
   "outputs": [],
   "source": [
    "# save final output\n",
    "# df_final_data.to_csv(data_filepath+'kc_housing_model_df_final_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SUMMARY FIGURE CODE FOR PRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.249801Z",
     "start_time": "2020-08-27T18:01:47.758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the visual style of the notebook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams.update(inline_rc)\n",
    "\n",
    "# inline_rc = dict(mpl.rcParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.250937Z",
     "start_time": "2020-08-27T18:01:47.764Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "# plt.style.use('dark_background')\n",
    "# plt.style.use('dark')\n",
    "\n",
    "\n",
    "# Define the figure and axes and the # of subplots, sharing the y axes\n",
    "fig, ax = plt.subplots(figsize=(16,12), ncols=2, nrows=2, sharey=True)\n",
    "\n",
    "## Defining Formatting to be Used\n",
    "\n",
    "# Formatting dollar sign labels\n",
    "fmtPrice = '${x:,.0f}'\n",
    "tickPrice = mtick.StrMethodFormatter(fmtPrice)\n",
    "\n",
    "# Axis Label fonts\n",
    "fontTitle = {'fontsize': 20,\n",
    "           'fontweight': 'bold',\n",
    "            'fontfamily':'serif'}\n",
    "\n",
    "fontAxis = {'fontsize': 16,\n",
    "           'fontweight': 'bold',\n",
    "            'fontfamily':'serif'}\n",
    "\n",
    "fontTicks = {'fontsize': 12,\n",
    "           'fontweight':'medium',\n",
    "            'fontfamily':'serif'}\n",
    "\n",
    "# The amount of space above titles\n",
    "y_title_margin = 1.01\n",
    "\n",
    "# Major title\n",
    "# plt.suptitle(\"Critical Factors for Predicting Sales Price\", y = 1.0, fontdict=fontTitle, fontsize=22)\n",
    "\n",
    "\n",
    "## Subplot 1\n",
    "i,j=0,0\n",
    "ax[i,j].set_title(\"Zipcode\",y = y_title_margin,fontdict=fontTitle)#, y = y_title_margin)\n",
    "sns.stripplot(df_final_data['zipcode'],df_final_data['price'],ax=ax[i,j],marker='o',size=3)\n",
    "\n",
    "# Remove xticks\n",
    "ax[i,j].set_xticks([]), ax[i,j].set_xlabel('')\n",
    "\n",
    "# Change y-tick labels\n",
    "ax[i,j].set_ylabel('Price',fontdict=fontAxis)\n",
    "\n",
    "yticklab = ax[i,j].get_yticklabels()\n",
    "ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "ax[i,j].get_yaxis().set_major_formatter(tickPrice)\n",
    "\n",
    "# Set y-grid\n",
    "ax[i, j].set_axisbelow(True)\n",
    "ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "\n",
    "\n",
    "## Subplot 2\n",
    "i,j = 0,1\n",
    "ax[i,j].set_title(\"Housing Grade\",y = y_title_margin,fontdict=fontTitle)\n",
    "ax[i,j].title.set_fontsize(20)\n",
    "sns.stripplot(df_final_data['grade'],df_final_data['price'],ax=ax[i,j],marker='o',size=2)\n",
    "\n",
    "#Set x axis\n",
    "xticklab=ax[i,j].get_xticklabels()\n",
    "ax[i,j].set_xticklabels(xticklab,fontdict=fontTicks)\n",
    "ax[i,j].set_xlabel('Grade')\n",
    "\n",
    "\n",
    "# Change y-tick labels\n",
    "ax[i,j].set_ylabel('')# 'Price',fontdict=fontAxis)\n",
    "\n",
    "# Set y-grid\n",
    "ax[i, j].set_axisbelow(True)\n",
    "ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "\n",
    "yticklab = ax[i,j].get_yticklabels()\n",
    "ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "ax[i,j].get_yaxis().set_major_formatter(tickPrice) \n",
    "\n",
    "\n",
    "## Subplot 3\n",
    "i,j = 1,0\n",
    "\n",
    "# Title\n",
    "ax[i,j].set_title(\"Living Space (sqft)\",y = y_title_margin,fontdict=fontTitle)\n",
    "ax[i,j].title.set_fontsize(20)\n",
    "\n",
    "# Define the scatter plot and line graph aesthetics\n",
    "line_kws={\"color\":\"orange\",\"alpha\":0.5,\"lw\":8,\"ls\":\":\"}\n",
    "scatter_kws={'s': 5, 'alpha': 0.5,'marker':'.','color':'red'}\n",
    "\n",
    "\n",
    "# Plot seaborn plot \n",
    "sns.regplot(df_final_data['sqft_living'], df_final_data['price'],ax=ax[i,j], scatter_kws=scatter_kws, line_kws=line_kws) #,marker='o',size=2) \n",
    "# sns.stripplot(df_final_data['sqft_living'], df_final_data['price'],ax=ax[i,j],marker='.') #,marker='o',size=2)\n",
    "\n",
    "## Change the x-axis \n",
    "ax[i,j].set_xlabel('Area (sqft)',fontdict=fontAxis)\n",
    "\n",
    "# Get ticks, rotate labels, and return\n",
    "# xticks = ax[i,j].get_xticks()\n",
    "xticklab=ax[i,j].get_xticklabels()\n",
    "ax[i,j].set_xticklabels(xticklab,fontdict=fontTicks, rotation=45)\n",
    "\n",
    "# Change the major units of x-axis\n",
    "ax[i,j].xaxis.set_major_locator(mtick.MultipleLocator(500))\n",
    "ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "## Change y-axis\n",
    "# Change y-tick labels\n",
    "ax[i,j].set_ylabel('Price',fontdict=fontAxis)\n",
    "\n",
    "yticklab = ax[i,j].get_yticklabels()\n",
    "ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "ax[i,j].get_yaxis().set_major_formatter(tickPrice) \n",
    "\n",
    "# Set y-grid\n",
    "ax[i, j].set_axisbelow(True)\n",
    "ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "\n",
    "\n",
    "# ## Subplot 4\n",
    "i,j = 1,1\n",
    "ax[i,j].set_title(\"Neighbor's Living Space (sqft)\",y = y_title_margin,fontdict=fontTitle)\n",
    "ax[i,j].title.set_fontsize(20)\n",
    "\n",
    "# Define the scatter plot and line graph aesthetics\n",
    "line_kws={\"color\":\"lime\",\"alpha\":0.5,\"lw\":8,\"ls\":\":\"}\n",
    "scatter_kws={'s': 5, 'alpha': 0.5,'marker':'.','color':'blueviolet'}\n",
    "\n",
    "# Plot seaborn plot \n",
    "sns.regplot(df_final_data['sqft_living15'], df_final_data['price'],ax=ax[i,j], scatter_kws=scatter_kws, line_kws=line_kws)\n",
    "\n",
    "# Change the x-axis labels\n",
    "ax[i,j].set_xlabel('Area (sqft)',fontdict=fontAxis)\n",
    "\n",
    "# Get ticks, rotate labels, and return\n",
    "xticklab=ax[i,j].get_xticklabels()\n",
    "ax[i,j].set_xticklabels(xticklab,fontdict=fontTicks, rotation=45)\n",
    "\n",
    "# Change the major units of x-axis\n",
    "ax[i,j].xaxis.set_major_locator(mtick.MultipleLocator(500))\n",
    "ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "# Change y-tick labels\n",
    "ax[i,j].set_ylabel('')#Price',fontdict=fontAxis)\n",
    "\n",
    "yticklab = ax[i,j].get_yticklabels()\n",
    "ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "ax[i,j].get_yaxis().set_major_formatter(tickPrice) \n",
    "\n",
    "# Set y-grid\n",
    "ax[i, j].set_axisbelow(True)\n",
    "ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_filepath+\"summary_figure.png\") # save as png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.252103Z",
     "start_time": "2020-08-27T18:01:47.772Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"Final Figures/map_median_price.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:01:53.254033Z",
     "start_time": "2020-08-27T18:01:47.777Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"Final Figures/map_latlong_price.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO: Tableau Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maps were created from final .csv loaded into Tableau Public and can be viewed and downloaded from  https://public.tableau.com/profile/james.irving#!/ \n",
    "\n",
    "#### Short how-to plot geo data in Tableau: \n",
    "- Load in your .csv dataset from your project.\n",
    "    - Let it use data interpreter. It should identify zipcode as a location.\n",
    "- On your worksheet page:\n",
    "    - **For plotting each price for each house:**\n",
    "        - Drag the Measures Lat and Long onto the rows and columns boxes (top of sheet)\n",
    "        - Drag the Measure price onto the Color Button under Marks. <br> It should now be listed at the bottom of the Marks panel. \n",
    "        - Right-click and select \"Dimension\"\n",
    "    - **For plotting median income by zipcode:**\n",
    "        - Drag zipcode form the Dimensions panel onto the main graph window. \n",
    "            - It will automatically load in map of location.\n",
    "        - Drag price onto the color button (it will now appear in the Marks window)\n",
    "        - Rich click on Price. Select \"Measure\" > Median\n",
    "    - **Customize map features** by selecting \"Map\" > Map Layers on the Menu Bar. \n",
    "    ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "216px",
    "width": "319px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents (Links)",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "309.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 420.031,
   "position": {
    "height": "442.016px",
    "left": "1573px",
    "right": "20px",
    "top": "-53px",
    "width": "643.563px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
